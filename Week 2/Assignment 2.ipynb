{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d37600b",
   "metadata": {},
   "source": [
    "# Turing Machine and Deep Learning 2023\n",
    "\n",
    "## Assignment 2: Supervised Classification Problems\n",
    "\n",
    "This notebook is meant for you to review and reflect on the content of Lecture 2, which was mainly about supervised learning problems in the general context of classification. We will reflect mainly on model and hyperparameter selection over the models we have discussed during the lecture: logistic regression, decision trees, random forests and support vector machines. This notebook should not be too much quantitative work (lines of code) but keep in mind that running this notebook may take a longer time than you may be used to for python programs (*training good models take time!*)\n",
    "\n",
    "### Handing in your Assignment\n",
    "\n",
    "Git is an invaluable resource to researchers and developers, and thus for this course, all course material will be (additionally) shared on GitHub. Though there is a tiny bit of a learning curve, this is worth the effort. To hand in your assignment (applicable to all weeks):\n",
    "\n",
    "1. Create a folder called \"Week 2\" and copy this notebook and any other files or data that may be needed.\n",
    "2. Finish the notebook and commit and push regularly. Your final commit before the deadline will be graded. \n",
    "\n",
    "\n",
    "### Grading\n",
    "\n",
    "Each one of the (sub-)questions below will be graded either 0 (insufficient), 1 (sufficient) or 2 (good). If $N$ is the number of sub-questions, and $p_i$ is your score for sub-question $i$, your total grade $G$ for this assignment is:\n",
    "$$G=\\frac{1}{2 N}\\sum_{i=0}^{N}p_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f0145",
   "metadata": {},
   "source": [
    "# Question 1 -- Classification Metrics\n",
    "\n",
    "To contextualise, we learnt of 4 metrics in class: accuracy, precision, recall and F1 score. Answer the following text questions in the following markdown cell.\n",
    "1. For accuracy, precision and recall, mention one specific example *each* (i.e. a well-defined problem where ML can be used) where they would be preferable.\n",
    "2. For accuracy, precision and recall, mention one specific example *each* (i.e. a well-defined problem where ML can be used) where they would fail.\n",
    "3. In which situations would the F1 score be helpful? Give one concrete example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6782a495",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffea6c",
   "metadata": {},
   "source": [
    "## Question 2 -- CIFAR 10\n",
    "\n",
    "As we've discussed last week, one of the prime issues with ML is figuring out what model you are going to use and when. In this case, we're going to use the [CIFAR-10](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10) dataset from TensorFlow, another benchmarking dataset. This is a considerably larger dataset, and contains coloured images. For the sake of computation time, we are just going to use the first 1000 training images but the full test set (though in a proper ML setting we would prefer to use all training data). The images are 32x32 coloured pixes. There are 10 labels which are integers by default. The dictionary `class_labels` translate them to their text label equivalents (referenced from [here](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10/load_data))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d29cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need to edit anything here,\n",
    "# just run this cell\n",
    "\n",
    "# get dataset\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "# get common libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm # this is a new one: makes loading bars\n",
    "\n",
    "def get_data_subset(n_train, n_test):\n",
    "    (train_X, train_y), (test_X, test_y) = cifar10.load_data()\n",
    "    train_X, train_y, test_X, test_y = train_X[:n_train], train_y[:n_train], test_X[:n_test], test_y[:n_test]\n",
    "    train_y, test_y = train_y.squeeze(), test_y.squeeze()\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "n_train, n_test = 1000, 1000\n",
    "\n",
    "train_X, train_y, test_X, test_y = get_data_subset(n_train, n_test)\n",
    "\n",
    "fig, axs = plt.subplots(5,5)\n",
    "axs = axs.flatten()\n",
    "fig.tight_layout(pad=0.3)\n",
    "\n",
    "class_labels = {0:\"airplane\",\n",
    "                1:\"automobile\",\n",
    "                2:\"bird\",\n",
    "                3:\"cat\",\n",
    "                4:\"deer\",\n",
    "                5:\"dog\",\n",
    "                6:\"frog\",\n",
    "                7:\"horse\",\n",
    "                8:\"ship\",\n",
    "                9:\"truck\"}\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.imshow(train_X[i])\n",
    "    ax.set_title(f\"{train_y[i]}:{class_labels[train_y[i]]}\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc3d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure the quantity of examples for each label are about the same\n",
    "plt.hist(train_y, bins=10, rwidth=0.9)\n",
    "plt.xticks(0.9*np.arange(10)+0.45, range(10))\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c5b605",
   "metadata": {},
   "source": [
    "## Question 2.1 -- Preprocessing\n",
    "\n",
    "The first thing we should do is preprocess the images so that they are ready to be input into our models. First, print out the shape of the dataset's inputs (`train_X`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8a16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a3622",
   "metadata": {},
   "source": [
    "**Q 2.1.1** What does each value in the tuple that was printed out mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec2a2bc",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401db68",
   "metadata": {},
   "source": [
    "**Q 2.1.2** Next, flatten the pixel values to a single vector. What is the length of this vector? Print out the shape of the flattened dataset's inputs once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79601de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "def flatten_cifar10(samples):\n",
    "    return ...\n",
    "\n",
    "train_X, test_X = ..., ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb7709",
   "metadata": {},
   "source": [
    "## Question 3 -- Model Selection using Grid Search\n",
    "\n",
    "In the lecture, we have seen four different ML classification algorithms: logistic regression, decision trees, random forests and support vector machines. In this assignment, we will figure out which is best, and a basic method to figure out the best way to tune the parameters of each one. Remember, a hyperparameter is something that you choose by hand about the model or the way it trains.\n",
    "\n",
    "### Question 3.1 -- Logistic regression\n",
    "\n",
    "Use `sklearn` (as in the lecture notebook) to classify the CIFAR10 sub-dataset. There are not a lot of major hyperparameters to tune here, so this exercise should be straightforward. Create a LogisticRegression object, fit it on the training data, and compute the train and test accuracies. \n",
    "\n",
    "*Expected runtime: ~1min*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118723f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "modelLR = ... # create the logistic regression object\n",
    "modelLR = ... # fit on training data\n",
    "print(\"Training score:\", ...)\n",
    "print(\"Testing score: \", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc3ea3",
   "metadata": {},
   "source": [
    "**Q 3.1.2** Does the model work well in your opinion? What about whether it is over/underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d9a2c",
   "metadata": {},
   "source": [
    "### Question 3.2 -- Decision Trees\n",
    "\n",
    "Decision trees have one main hyperparameter that you can tune -- this is the maximum depth of the tree being trained. Thus, we'll try and figure out what depth is the optimal for our purposes.\n",
    "\n",
    "An important thing to note is that decision trees are randomized initially -- this means that two trees of the same depth may have wildly different performances, depending on how they were initialized.\n",
    "\n",
    "**Q 3.2.1** Loop over max tree depths from $d=1$ to $d=15$ and store the mean and standard deviation of train and test scores for 10 randomly initialised trees. \n",
    "\n",
    "*Expected runtime: <5 mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d852e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "train_acc_mean, train_acc_std = [], [] # to store the training accuracies\n",
    "test_acc_mean, test_acc_std = [], []   # to store the testing accuracies\n",
    "\n",
    "for d in tqdm(range(...)): # loop over tree depths\n",
    "    train_perfs = [] # store interem train scores\n",
    "    test_perfs = []  # store interem test scores\n",
    "    for n in range(...):   # loop over random initializations\n",
    "        # init new model\n",
    "        # train model\n",
    "        # store interem values\n",
    "    \n",
    "    # append mean and std scores to appropriate lists\n",
    "    train_acc_mean.append(...)\n",
    "    train_acc_std.append(...)\n",
    "    test_acc_mean.append(...)\n",
    "    test_acc_std.append(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe9507a",
   "metadata": {},
   "source": [
    "**Q 3.2.2** Plot the test and train means with errorbars equal to one standard deviation (look at [plt.errorbar](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.errorbar.html) for reference). Don't forget to add labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91598d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(...)\n",
    "plt.errorbar(...)\n",
    "plt.title(...)\n",
    "plt.ylabel(...)\n",
    "plt.xlabel(...)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5829105",
   "metadata": {},
   "source": [
    "**Q 3.2.3**\n",
    "\n",
    "1. Is there a lot of variation of the performance of the trees (enough to question statistical significance)?\n",
    "2. Which model (of the ones you tested) performed best on the training set? Which performed best on the test set?\n",
    "3. At what depth do models start to overfit the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5144dd0",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071f96d",
   "metadata": {},
   "source": [
    "## Question 3.3 -- Random Forests\n",
    "Just like in the decision trees, one hyperparameter you can choose is the depth of the tree. However, another important one is the number of decision trees. In this case, the performance is affected by the combination of these hyperparameters, and so we need to train and evaluate them at each combination. \n",
    "\n",
    "**Q 3.3.1** Loop over $n_{trees}=10$ to $n_{trees}=100$ in increments of 10, and max tree depths from $d\\in[1,3,5,7,9,11,13,15]$ and store the mean and standard deviation of train and test scores for 5 randomly initialised trees.\n",
    "\n",
    "*Expected runtime: ~9 minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e32e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "train_acc_mean, train_acc_std = np.zeros((10, 8)), np.zeros((10, 8)) # storing it in an np array instead of a list makes it easier\n",
    "test_acc_mean, test_acc_std = np.zeros(...), np.zeros(...)  \n",
    "\n",
    "n_trees = [...]\n",
    "depths = [...]\n",
    "\n",
    "for tree_idx, n_trees in tqdm(enumerate(n_trees), total=len(n_trees)):\n",
    "    for d_idx, d in enumerate(depths):\n",
    "        train_perfs = []\n",
    "        test_perfs = []\n",
    "        for n in range(...):\n",
    "            # init new model\n",
    "            # train model\n",
    "            # store interem values\n",
    "        train_acc_mean[tree_idx][d_idx] = ...\n",
    "        train_acc_std[tree_idx][d_idx] = ...\n",
    "        test_acc_mean[tree_idx][d_idx] = ...\n",
    "        test_acc_std[tree_idx][d_idx] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffad902",
   "metadata": {},
   "source": [
    "Looping over several variables and testing each combination is called a **grid search**. Since we have two hyperparameters, we cannot plot a 1D line to see which is best, as we did for normal DTs. One way to visualize this instead is a heatmap. For this, we can `seaborn`'s [heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) function.  \n",
    "\n",
    "**Q 3.3.2** Plot two heatmaps: one for the train accuracies and one for the test ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ec439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Figure 1\n",
    "plt.figure()\n",
    "sns.heatmap(..., annot=True) # create seaborn heatmap with annotations\n",
    "# add proper xticklabels and yticklabels\n",
    "# add a title\n",
    "...\n",
    "plt.show()\n",
    "\n",
    "# Figure 2\n",
    "plt.figure()\n",
    "# create seaborn heatmap with annotations\n",
    "# add proper xticklabels and yticklabels\n",
    "# add a title\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490208a8",
   "metadata": {},
   "source": [
    "**Q 3.3.2**\n",
    "1. What trends with respect to each hyperparameter do the heatmaps show you? \n",
    "2. What model performs best on the train set? What model performs best on the test set?\n",
    "3. What model would you choose to deploy and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00fd6ea",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c11241",
   "metadata": {},
   "source": [
    "## Question 3.4 -- Support Vector Machines\n",
    "\n",
    "`sklearn`'s SVM classifier implementation (called \"SVC\", we already met them in the lecture) contains quite a number of hyperparameters you can tune. The ones we are looking at today are the kernel, the parameter `C` (which is some penalty term for incorrectly classifying a data point, applicable to the RBF kernel), `gamma`, which is a measure of how important closer points to the decision boundary are with respect to the decision boundary (applicable to the RBF kernel), and `degree`, which is the degree of the polynomial function (applicable to the poly kernel). Let $C\\in\\{0.1, 1, 10, 100\\}$ and $gamma\\in\\{10^x|-2\\leq x\\leq 2, x\\in\\mathbb{Z}\\}$. Let us use $degree\\in\\{2,3,4\\}$.\n",
    "\n",
    "A note about the kernel: In short, this is a function that determines what the shape of the decision boundary are. The choices that we can check out here (there are more) are `linear` (linear decision boundary), `poly` (polynomial) and `rbf` (radial basis function) in order of flexibility.\n",
    "\n",
    "Since we are trying to optimize for a number of different parameters, writing out all the code ourselves gets a bit messy. Instead, we use `sklearn` again! The class in question is called `GridSearchCV`, which performs a grid search over parameters with specified values. \n",
    "\n",
    "The 'CV' part of the name refers to the fact that we are performing *cross-validation*, which is related to the concept of validation sets that we encountered last week. We will cover it in a future lecture, but for now, the way it works is that it splits the training set into $k$ sets (called 'folds') and iteratively trains on $k-1$ folds and validates on the remaining one. The result is the mean over $k$ iterations. CV is considered the 'gold standard' with respect to analysing model robustness. We will use $k=5$ (appropriately called *5-fold cross-validation*), which is standard for a first analysis.\n",
    "\n",
    "**Q 3.4.1** Run a gridsearch with 5-fold cross-validation over the hyperparameters discussed above to find an optimal SVC model. Please look at GridSearchCV's documentation for more information.\n",
    "\n",
    "*Expected runtime: Around 15 minutes if you're lucky.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbccfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid_svm = [\n",
    "    {'kernel': [...],\n",
    "     'C': [...],\n",
    "     'degree': [...],\n",
    "     'gamma': [...]}\n",
    "]\n",
    "\n",
    "# create the cross-validation object\n",
    "optimal_params_svm = GridSearchCV(\n",
    "    estimator = SVC(),             # support vector classifier instance\n",
    "    param_grid = ...,              # grid search params\n",
    "    cv = ...,                      # k=5 CV\n",
    "    scoring='accuracy',            # use accuracy measure for best hyperparameters\n",
    ")\n",
    "\n",
    "optimal_params_svm.fit(...)\n",
    "\n",
    "print(\n",
    "    \"The best parameters are %s with a score of %0.2f\"\n",
    "    % (optimal_params_svm.best_params_, optimal_params_svm.best_score_)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6997c6",
   "metadata": {},
   "source": [
    "**Q 3.4.2** \n",
    "1. How many combinations of parameters are there? Why?\n",
    "2. Using 5-fold validation, how many models do you train in total?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f5cb8",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906bf380",
   "metadata": {},
   "source": [
    "**Q 3.4.3** \n",
    "Create a model `modelSVC` with the optimal parameters you found above and print out the train and test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSVC = SVC(...) # create model\n",
    "modelSVC = ...      # train model\n",
    "print(\"Training score:\", ...)\n",
    "print(\"Testing score: \", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71733efc",
   "metadata": {},
   "source": [
    "## Question 4 -- Training and inference speed\n",
    "Depending on the application, the choice of model may be affected by more than just the raw score. One of these factors is *time* -- specifically, how long does the model take to train, and how long does it take to make a prediction. In this case we're increasing the number of training points to 5000.\n",
    "\n",
    "**Q 4.1** Create models with the  that you found above, measure how long it takes for it to train and classify images from the test and train set, along with their accuracies (a classification, or a single run of a model, is generally called an 'inference'). We generally store time with an action taken (fit or inference) *per image* and the inference times *per image*.\n",
    "\n",
    "*Runtime: ~3mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497bf716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter # used to compute intervals\n",
    "\n",
    "n_train, n_test = ..., ...\n",
    "... = get_data_subset(...) # get data\n",
    "... # flatten images\n",
    "\n",
    "# set up lists to store scores and times\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "fit_times = []\n",
    "inf_times = []\n",
    "\n",
    "def train_and_eval(model, train_X, train_y, test_X, test_y):\n",
    "    \"\"\" Trains, times and evaluates a given instantiated model on data \"\"\"\n",
    "    \n",
    "    start = perf_counter()               # check current (start) time\n",
    "    model = model.fit(...)               # perform some code that you want to time\n",
    "    end = perf_counter()                 # check current (end) time\n",
    "    fit_time = end-start                 # compute interval\n",
    "    \n",
    "    # time inference over train score\n",
    "    start = perf_counter()\n",
    "    train_score = ...\n",
    "    end = perf_counter()\n",
    "    train_score_time = end-start\n",
    "    \n",
    "    # time inference over test score\n",
    "    start = perf_counter()\n",
    "    test_score = ...\n",
    "    end = perf_counter()\n",
    "    test_score_time = end-start\n",
    "    \n",
    "    return train_score, test_score, fit_time, train_score_time, test_score_time\n",
    "\n",
    "# create models\n",
    "modelLR = ...\n",
    "modelDT = ...\n",
    "modelRFC = ...\n",
    "modelSVC = ...\n",
    "\n",
    "for model in tqdm([modelLR, modelDT, modelRFC, modelSVC]):\n",
    "    train_score, test_score, fit_time, train_score_time, test_score_time = train_and_eval(model, train_X, train_y, test_X, test_y)\n",
    "    # add these values to the list set up above\n",
    "    # don't forget that the training and inference times should be stored per image and not for the whole run  \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35abc2c2",
   "metadata": {},
   "source": [
    "Plot two barplots:\n",
    "1. **Q 4.2** On one, plot a dual barplot showing the test and train accuracies.\n",
    "2. **Q 4.3** On the other, plot a double barplot showing the training time *per image* and the inference times *per image* (so divide the total time by the number of images).\n",
    "\n",
    "For each, make sure you don't forget the unit of measurement, add a title and axis labels and make sure there are labels and a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ac5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [...]\n",
    "bar_width = 0.35\n",
    "\n",
    "# Create an array for the x-axis positions\n",
    "x = np.arange(...)\n",
    "\n",
    "# Plotting the bars\n",
    "fig, ax = plt.subplots()\n",
    "bar1 = ax.bar(...) # training accuracy: don't forget to add labels\n",
    "bar2 = ax.bar(...) # testing accuracy\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel(...)\n",
    "ax.set_ylabel(...)\n",
    "ax.set_title(...)\n",
    "# set proper x ticks: I'll help you with this one\n",
    "ax.set_xticks(x + bar_width / 2)\n",
    "ax.set_xticklabels(model_names)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fed16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the times\n",
    "fig, ax = plt.subplots()\n",
    "bar1 = ax.bar(...) # fit times: don't forget labels\n",
    "bar2 = ax.bar(...) # inference times\n",
    "\n",
    "# Add labels, title, and legend\n",
    "# set proper x ticks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d8c37",
   "metadata": {},
   "source": [
    "**Q 4.4**\n",
    "1. What model is the fastest to train?\n",
    "2. What model is fastest to infer?\n",
    "3. What model has the highest train accuracies? What model has the highest test accuracies?\n",
    "4. What model would you choose to use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2f076e",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
