{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43554d37",
   "metadata": {},
   "source": [
    "# Turing Machine and Deep Learning 2023\n",
    "\n",
    "## Assignment 4 -- Neural Networks\n",
    "\n",
    "This notebook is meant for you to review and reflect on the content of Lecture 4, which was mainly about neural networks. In particular, we'll focus on aspects that were only briefly mentioned in class in order for you to get a better understanding of it. \n",
    "\n",
    "### Handing in your Assignment\n",
    "\n",
    "Git is an invaluable resource to researchers and developers, and thus for this course, all course material will be (additionally) shared on GitHub. Though there is a tiny bit of a learning curve, this is worth the effort. To hand in your assignment (applicable to all weeks):\n",
    "\n",
    "1. Create a folder called \"Week 4\" and copy this notebook and any other files or data that may be needed.\n",
    "2. Finish the notebook and commit and push regularly. Your final commit before the deadline will be graded. \n",
    "\n",
    "\n",
    "### Grading\n",
    "\n",
    "Each one of the (sub-)questions below will be graded either 0 (insufficient), 1 (sufficient) or 2 (good). If $N$ is the number of sub-questions, and $p_i$ is your score for sub-question $i$, your total grade $G$ for this assignment is:\n",
    "$$G=\\frac{1}{2 N}\\sum_{i=0}^{N}p_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fbab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load common libraries\n",
    "import numpy as np                 # maths\n",
    "import matplotlib.pyplot as plt    # plotting\n",
    "import pandas as pd                # data manipulation\n",
    "from tqdm import tqdm              # loading bar\n",
    "from time import perf_counter      # timer\n",
    "import tensorflow as tf            # NNs and associated\n",
    "from tensorflow import keras       # NNs and associated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c3a0a",
   "metadata": {},
   "source": [
    "# Q1 CNNs\n",
    "\n",
    "In the lecture, we saw some code that implemented MLPs and CNNs for the task of image classification on CIFAR10. However, we skipped over some details of what CNNs actually do.\n",
    "\n",
    "A CNN works by automatically learning and extracting meaningful patterns, features, and hierarchies from the input data in the following way:\n",
    "\n",
    "1. **Convolution**: A CNN applies (multiple) small filters (kernels) to an input image, performing element-wise multiplications and sums to extract features like edges, corners, and textures. We also did this computation in person during class. These kernels may be a matrix (what we saw in the lecture), or even a stack of matrices (e.g. for RGB images). I'd recommend watching this [3Blue1Brown video](https://www.youtube.com/watch?v=KuXjwB4LzSA&pp=ygUTY29udm9sdXRpb25zIGluIGNubg%3D%3D) and this [DeepLearning.AI](https://www.youtube.com/watch?v=KTB_OFoAQcc) video for reference.\n",
    "2. **Non-linearity**: Non-linear activation functions (e.g., ReLU) are applied to introduce complexity and allows the capture of more abstract representations.\n",
    "3. **Pooling**: Pooling operations are used to downsample the spatial dimensions of feature maps while preserving the most important information. In both cases, a pooling window or filter moves across the input feature map, and a single value is selected or computed within the window based on the pooling operation. Pooling helps reduce the computational complexity of the network and introduces some translation invariance. There are a few types of pooling, but most commonly used are max-pooling and average-pooling.\n",
    "4. **Hierarchy**: Convolution and pooling operations are repeated to create a hierarchy of feature extraction layers, learning more complex and high-level features.\n",
    "5. **Fully Connected Layers**: Flattened features are passed through fully connected layers, similar to a traditional MLP, for classification or regression.\n",
    "6. **Training**: Parameters are adjusted using backpropagation and optimization algorithms to minimize the difference between predictions and true labels.\n",
    "\n",
    "By leveraging convolution, non-linearity, and hierarchical feature extraction, CNNs can automatically learn and recognize intricate patterns and structures in visual data, making them well-suited for tasks like image classification and object detection.\n",
    "\n",
    "You can think of the convolution operation with a kernel over an image as outputting a new modified image, called a feature map. When applying convolutions over feature maps, the size of the output feature map (in each dimension) can be computed by:\n",
    "\n",
    "$$o = \\frac{i - k + 2 p}{s}+1$$\n",
    "\n",
    "where the hyperparameters of the convolution are:\n",
    "- $i$: input_size is the size (height or width) of the input feature map.\n",
    "- $k$: kernel_size is the size (height or width) of the convolutional kernel/filter.\n",
    "- $p$: padding is the number of pixels added to each side of the input feature map (if applicable) (default 0)\n",
    "- $s$: stride is the step size or the number of pixels the kernel moves at each step (default 1)\n",
    "\n",
    "The output size of a pooling operation can also be defined as:\n",
    "$$o = \\frac{i - p}{s}+1$$\n",
    "\n",
    "For both of these above computations, the outputs (if they are floating points) are *floored*, i.e. rounded down to the nearest positive integer (since you cannot have a non-negative number of pixels).\n",
    "\n",
    "**Q1.1** Thus, starting with a 28x28 pixel image (e.g. MNIST), write a series of kernel sizes to use in convolutions (optionally including a pooling operation if you want to try it out, and optionally padding and stride) in order to get it down to an image size of 6x6. There are several correct answers possible here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92986362",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5510986",
   "metadata": {},
   "source": [
    "**Q1.2** Now, let's try out your strategy in code. Implement the convolutions you defined above to perform classification over MNIST digits. Note that most of the code is written for you, but be mindful of the steps that are written. Note that you need to choose the number of kernals as well as the size at each layer. The input shape of your next layer is the output shape you computed above and the number of kernels you chose for the previous layer ``input_shape=(w,h,k)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae273e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd251c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this cell\n",
    "\n",
    "# A Conv2D layer performs 2D convolution -- the same as we saw in class.\n",
    "# The main inputs to this class are the number of kernels, the size of each kernel,\n",
    "# the non-linear activation function and the input shape. Optionally, you can define a padding\n",
    "# and/or a stride. Use it like this:\n",
    "# keras.layers.Conv2D(<n_kernels>, \n",
    "#                     kernel_size=<tuple>, \n",
    "#                     activation=<string>, \n",
    "#                     input_shape=<tuple>, \n",
    "#                     padding=<string>, \n",
    "#                     stride=<int/tuple>)\n",
    "# You can read the documentation here: https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "\n",
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(..., kernel_size=..., activation=..., input_shape=(28, 28, 1)), # i'll give you the first shape for free\n",
    "    ...                                                # add more layers here if you need to\n",
    "    keras.layers.Flatten(),                            # flatten to dense layer for classification\n",
    "    keras.layers.Dense(..., activation=...),           # add dense layer (add more if you want) \n",
    "    keras.layers.Dense(..., activation='softmax')      # final dense layer (how many categories are there?). \n",
    "                                                       # softmax for probability distribution output\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model: choose batch size and number of epochs\n",
    "history = model.fit(x_train, y_train, batch_size=..., epochs=..., validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(...)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b2f4c",
   "metadata": {},
   "source": [
    "**Q1.3** Plot the train and validation losses and and accuracies. Do not forget to add labels What can you say about the model over/underfitting? Would you continue training it, or stop earlier? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62559dd",
   "metadata": {},
   "source": [
    "**Your text answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd24bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.plot(history.history[\"loss\"], label=...) # plot loss\n",
    "plt.plot(history.history[...], label=...) # plot val_loss\n",
    "plt.ylabel(...)\n",
    "plt.xlabel(...) # x-axis=epochs\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title(...)\n",
    "plt.show()\n",
    "\n",
    "# plot accuracies\n",
    "plt.plot(...) # plot accuracy\n",
    "plt.plot(...) # plot val_accuracy\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f0d4c",
   "metadata": {},
   "source": [
    "# Q2 RNNs\n",
    "\n",
    "An RNN (Recurrent Neural Network) is a type of artificial neural network that is particularly effective in processing sequential data. Unlike feedforward neural networks, which process inputs independently, RNNs have connections that allow information to flow in cycles. This cyclic connectivity enables them to capture temporal dependencies and learn patterns over time. You can find more in these [neural network lecture notes (Jaeger 2023, chapter 4)](https://www.ai.rug.nl/minds/uploads/LN_NN_RUG.pdf). Here is an overview of how RNNs work:\n",
    "\n",
    "- **Recurrent Connections**: RNNs have recurrent connections that allow information to be passed from one step to the next in a sequence. At each time step, the RNN takes an input and combines it with the information from the previous step. This feedback loop enables the network to have memory and make predictions based on the context of past inputs.\n",
    "\n",
    "- **Hidden State**: RNNs maintain a hidden state vector that serves as a memory of the network. The hidden state is updated at each time step and contains information about the previous inputs in the sequence. It captures the network's understanding of the sequence up to that point and is used to influence the processing of future inputs.\n",
    "\n",
    "- **Sequence Processing**: RNNs process sequences by iterating through each element one at a time. As the network receives an input at each time step, it updates its hidden state based on the input and the previous hidden state. The updated hidden state is then used to make predictions or generate outputs.\n",
    "\n",
    "- **Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU)**: LSTMs and GRUs are popular types of RNN architectures that address the vanishing gradient problem and capture long-term dependencies more effectively. These architectures introduce specialized memory cells and gating mechanisms that allow the network to selectively update and forget information.\n",
    "\n",
    "- **Training**: RNNs are typically trained using the backpropagation through time (BPTT) algorithm, which extends backpropagation to handle sequences. The goal is to minimize the difference between the predicted outputs and the true targets by adjusting the network's parameters through gradient descent optimization.\n",
    "\n",
    "- **Applications**: RNNs are well-suited for tasks involving sequential data, such as natural language processing, speech recognition, machine translation, time series analysis, and sentiment analysis. They can effectively model dependencies and capture context in these domains.\n",
    "\n",
    "By leveraging recurrent connections and hidden state information, RNNs excel at processing and understanding sequential data. They are powerful tools for tasks that require temporal modeling and have made significant contributions to the field of deep learning.\n",
    "\n",
    "Here, we'll use LSTMs to regress on the [Air Passengers](https://www.kaggle.com/datasets/chirag19/air-passengers) dataset, providing monthly totals of a US airline passengers from 1949 to 1960."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the Air Passengers dataset\n",
    "data = pd.read_csv('AirPassengers.csv')\n",
    "time_series = data['#Passengers'].values.astype(float)\n",
    "\n",
    "# preprocess, use a scaler to scale the features to 0 and 1\n",
    "time_series = np.array(time_series).reshape(-1,1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "time_series = scaler.fit_transform(time_series)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = time_series[:120]  # First 120 months for training\n",
    "test_data = time_series[120:]   # Last 24 months for testing\n",
    "\n",
    "# Function to create input sequences\n",
    "# Goal: Predict the (n+1)th point given n points\n",
    "def create_sequences(data, seq_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define sequence length and create input sequences\n",
    "sequence_length = 6\n",
    "X_train, y_train = create_sequences(train_data, sequence_length)\n",
    "X_test, y_test = create_sequences(test_data, sequence_length)\n",
    "\n",
    "# Reshape the input data to be 3D (batch_size, sequence_length, num_features)\n",
    "# Standard practice!\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7767c7",
   "metadata": {},
   "source": [
    "**Q2.1** Finish the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9db72a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.LSTM(..., input_shape=(sequence_length, 1)),\n",
    "    ... # add more layers if you want\n",
    "    keras.layers.Dense(...) # how many outputs in this regression task? \n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss=..., optimizer='adam') # use MSE loss and Adam optimizer. If you don't know how, look it up!\n",
    "history = model.fit(..., ..., epochs=..., batch_size=..., validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aebc2f",
   "metadata": {},
   "source": [
    "**Q2.2** Plot the train and validation losses and and accuracies. Do not forget to add labels What can you say about the model over/underfitting? Would you continue training it, or stop earlier? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858b1c6",
   "metadata": {},
   "source": [
    "**Your text answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e3f6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919f939",
   "metadata": {},
   "source": [
    "Now, we can make predictions on our test set and evaluate it!\n",
    "\n",
    "**Q2.3** Finish the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "# make predictions\n",
    "train_preds = model.predict(...)\n",
    "test_preds = ...\n",
    "\n",
    "# invert predictions to make it original scale\n",
    "train_preds = scaler.inverse_transform(...)\n",
    "y_train = scaler.inverse_transform(...)\n",
    "test_preds = ...\n",
    "y_test = ...\n",
    "\n",
    "# plotting code adapted from https://www.kaggle.com/code/singhalamogh/lstm-regression-on-time-series-data\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(data)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[sequence_length:len(train_preds) + sequence_length, :] = train_preds\n",
    "\n",
    "# shift test true data for plotting\n",
    "testTruePlot = np.empty_like(data)\n",
    "testTruePlot[:, :] = np.nan\n",
    "testTruePlot[len(train_preds)+sequence_length:len(data), :] = scaler.inverse_transform(test_data)\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(data)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(train_preds)+2*sequence_length:len(data), :] = test_preds\n",
    "\n",
    "# plot baseline and predictions\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plt.plot(scaler.inverse_transform(train_data), color=colors[0], label=\"Train data\")\n",
    "plt.plot(testTruePlot[:,0], color=colors[1], label=\"Test data\")\n",
    "plt.plot(trainPredictPlot[:,0], color=colors[0], linestyle=\"--\", label=\"Predictions based on train data\")\n",
    "plt.plot(testPredictPlot[:,0], color=colors[1], linestyle=\"--\", label=\"Predictions based on test data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# calculate root mean squared error\n",
    "train_score = mean_squared_error(y_train, train_preds)\n",
    "print('Train Score: %.2f (MSE)' % (train_score))\n",
    "test_score = mean_squared_error(y_test, test_preds)\n",
    "print('Test Score: %.2f (MSE)' % (test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f95144",
   "metadata": {},
   "source": [
    "**Q2.4** Does the model do well? Write three ideas on how you would make it better. Implement at least one of these ideas and rerun your code, and report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43335a9",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b2e6a2",
   "metadata": {},
   "source": [
    "# Q3 Full pipeline\n",
    "\n",
    "In lieu of this being the final assignment, I would like to drill down last week's final question as being the final question of the course assignments. \n",
    "\n",
    "Now that you know about neural networks and regularization (early stopping, dropout, etc), try and predict the genre of the movie based on the overview. \n",
    "- This week, you may use the overview, as well as any other data from the dataframe that you'd like, processed how you'd like. Note that the inputs to your model **must not** contain the genres themselves. \n",
    "- Copy your assignment notebook and data set from last week into the current folder (Week 4 -- **do not** edit the file in folder Week 3).\n",
    "- Focus on the final question (please let me/the TA's know if you need help with previous questions from last week). \n",
    "- Attempt to maximize your test score and don't forget to time the model training and inference (look at Assignment 2 for reference on how to do this). \n",
    "- Use whatever kind of UML and SML methods you'd like, including the networks you learned about this week (I'd suggest MLPs or RNNs). \n",
    "- Fill in the 150 word paragraph at the very end of the file.\n",
    "- Finally, when you're done, submit to the competition with your final result. You can submit multiple times before the next lecture. We'll discuss methods and results during the final lecture to see how you and your peers have done.\n",
    "\n",
    "This question will be graded pass or fail depending on if you've made an honest attempt at it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
