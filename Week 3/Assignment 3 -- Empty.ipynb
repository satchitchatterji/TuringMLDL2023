{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43554d37",
   "metadata": {},
   "source": [
    "# Turing Machine and Deep Learning 2023\n",
    "\n",
    "## Assignment Unupervised ML\n",
    "\n",
    "This notebook is meant for you to review and reflect on the content of Lecture 3, which was mainly about unsupervised learning problems. As with last week, this notebook should not be too much quantitative work (lines of code) but keep in mind that running this notebook may take a longer time than you may be used to for python programs (*training good models take time!*) \n",
    "\n",
    "### Handing in your Assignment\n",
    "\n",
    "Git is an invaluable resource to researchers and developers, and thus for this course, all course material will be (additionally) shared on GitHub. Though there is a tiny bit of a learning curve, this is worth the effort. To hand in your assignment (applicable to all weeks):\n",
    "\n",
    "1. Create a folder called \"Week 2\" and copy this notebook and any other files or data that may be needed.\n",
    "2. Finish the notebook and commit and push regularly. Your final commit before the deadline will be graded. \n",
    "\n",
    "\n",
    "### Grading\n",
    "\n",
    "Each one of the (sub-)questions below will be graded either 0 (insufficient), 1 (sufficient) or 2 (good). If $N$ is the number of sub-questions, and $p_i$ is your score for sub-question $i$, your total grade $G$ for this assignment is:\n",
    "$$G=\\frac{1}{2 N}\\sum_{i=0}^{N}p_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fbab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load common libraries\n",
    "import numpy as np                 # maths\n",
    "import matplotlib.pyplot as plt    # plotting\n",
    "import pandas as pd                # data manipulation\n",
    "from tqdm import tqdm              # loading bar\n",
    "from time import perf_counter      # timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c3a0a",
   "metadata": {},
   "source": [
    "# Q1 Loading and preprocessing data\n",
    "\n",
    "In this repository, you should find a file called `tmdb_5000_movies.csv` which is information from a subset of movies on The Movie Database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40715541",
   "metadata": {},
   "source": [
    "### Q.1.1 \n",
    "Use pandas to read in the csv file (refer to [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) if you're unfamiliar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10a5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(...)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1915f1f7",
   "metadata": {},
   "source": [
    "View information about the dataset including datatypes and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c173d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64a24f",
   "metadata": {},
   "source": [
    "**Q.1.1.2** What columns have null values above? How would you be able to estimate the missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2660fe51",
   "metadata": {},
   "source": [
    "*Your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83863820",
   "metadata": {},
   "source": [
    "### Q1.2\n",
    "For this notebook, we're only interested in a few columns, specifically title, overview and genres. Set df to only contain those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[...]\n",
    "df.info() # print out df info to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b596e86b",
   "metadata": {},
   "source": [
    "### Q1.3\n",
    "We see that some movies do not have an overview. Drop them from the table and reset the index (set `drop=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e4f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ... # drop na values\n",
    "df = ... # reset index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759df3a2",
   "metadata": {},
   "source": [
    "# Q2 One-hot encodings\n",
    "The following code processes the genres (which you see above are list of dictionaries) and encodes them into one-hot labels.\n",
    "\n",
    "Remember, since we cannot do maths on text strings, we must change these words to numbers. Here, we create one-hot encodings for the genres. Assume we have three genres $[G_1, G_2, G_3]$. If a movie $M_1$ is tagged with genre $G_1$, the encoding is $[1,0,0]$. If another movie $M_2$ is tagged with $G2$, the encoding becomes $[0,1,0]$. Other encodings are possible as well, but one-hot encoding quite common and useful for many other tasks. In this case, if a movie is tagged with more than one genre, we'll just take a random one and encode that into a one-hot label.\n",
    "\n",
    "### Q2.1 Processing genres\n",
    "\n",
    "Finish the following code that processes genres into one-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba05feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def process_genre_string(gs):\n",
    "    \"\"\" Processes genres into a single item from a list\"\"\"\n",
    "    gs = eval(gs)\n",
    "    gs = [x['name'] for x in gs]\n",
    "    genre = \"Unknown\"\n",
    "    if gs:\n",
    "        genre = np.random.choice(gs) # choose random entry\n",
    "    return genre\n",
    "\n",
    "# TODO: Fill in\n",
    "def encode_labels(label, unique_labels):\n",
    "    \"\"\" Encodes text labels into a one-hot encoded list\n",
    "        with possibly more than one one-hot per list.\n",
    "        :param label: the label you want to one-hot encode\n",
    "        :unique_labels: the vocabulary\n",
    "    \"\"\"\n",
    "    out = np.zeros(...)\n",
    "    out[...] = ...\n",
    "    return out\n",
    "\n",
    "# save genres\n",
    "processed_genres = []\n",
    "for index, row in df.iterrows():\n",
    "    processed_genres.append(...)\n",
    "\n",
    "# sort and remove duplicates to get vocabulary\n",
    "unique_genres = sorted(list(set(processed_genres)))\n",
    "\n",
    "# add to dataframe as new column\n",
    "df[\"proc_genres\"] = processed_genres\n",
    "\n",
    "# one-hot encode genres\n",
    "enocded_labels = []\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    enocded_labels.append(...)\n",
    "\n",
    "df[\"one_hot_genres\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17501a9",
   "metadata": {},
   "source": [
    "Verify that one-hot encodings are indeed as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"one_hot_genres\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a83ece",
   "metadata": {},
   "source": [
    "### Q2.2 Processing Overviews\n",
    "Next, we process the overviews. First, we remove all punctuation for the sake of simplicity and change each overview to use only lowercase. Then we need to see how long the overviews are in terms of numbers of words. We can do this with a histogram.\n",
    "\n",
    "**Q2.2.1** Finish the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd51c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_punctuation(text):\n",
    "    \"\"\" Only retains letters, numbers, underscores and whitespace \"\"\"\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "processed_overviews = []\n",
    "\n",
    "# loop over df, remove punctuation, make words lowercase and add it to a new column\n",
    "...\n",
    "df[\"proc_overview\"] = ...\n",
    "\n",
    "# get the word lengths of each overview and store it in a list \n",
    "overview_lens = []\n",
    "...\n",
    "\n",
    "# for the sake of simplicity, add these values as a column to the df\n",
    "df[\"overview_len\"] = ...\n",
    "plt.hist(...)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58138594",
   "metadata": {},
   "source": [
    "**Q2.2.2** What is the shortest overview? What is the longest? In the next step, we need to decide on a standard length of all overviews -- this means dropping overviews less than some value, and truncating longer ones. What length would you choose to minimize the number of movies dropped and maximize the information (words) stored in the overviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87f1af",
   "metadata": {},
   "source": [
    "*Your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e6cb90",
   "metadata": {},
   "source": [
    "### Q2.2.3\n",
    "Let's choose overviews of length 15 words. What this means is we need to discard movies that have overviews less than 15 (there are other ways of dealing with it, but this should be fine for now) and truncate the higher ones to the first 15 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70854211",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len = 15 # number of words\n",
    "# only select rows where overview len is more than or equal to token_len\n",
    "df = df[...]\n",
    "\n",
    "# split each proc_overview into a list of words, select the first token_len words, \n",
    "# and add the list of words back into df[\"proc_overview\"]\n",
    "df[\"proc_overview\"] = ...\n",
    "\n",
    "# print to verify\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e092dd",
   "metadata": {},
   "source": [
    "### Q2.2.3 Finding the vocabulary length\n",
    "\n",
    "In order to one-hot encode words, we need to find how many words there are in total, just like in the case of genres. Get all the words, remove duplicates, and sort. Find and print the length of your vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: store all words for all movies in a set, change it to a list and sort\n",
    "\n",
    "vocab_len = len(...)\n",
    "print(vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276b825",
   "metadata": {},
   "source": [
    "### Q2.2.4 Encoding the labels\n",
    "\n",
    "In the case of genres, we one-hot encoded the outputs by taking a single random genre. However, an alternative method is to add up the one-hot encodings to form some kind of histogram. For example, if we have an overview \"a brown dog\", and our vocab is \\[a brown, big, cabbage, dog, goat, cow, turkey\\], the one-hot vector would be \\[1,1,0,0,1,0,0,0\\]. If our overview is \"a big brown dog\", the one-hot vector would be \\[1,1,1,0,1,0,0,0\\]. If our overview is \"a big big brown dog\", the one-hot vector would be \\[1,1,2,0,1,0,0,0\\]. You can use the `encode_labels` function that you defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d212192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is just a hint, if you want you can do it as you please\n",
    "# as long as the output remains the same\n",
    "encoded_labels = []\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    sentence_encode = [] # set of encodings for this overview\n",
    "    for word in row[\"proc_overview\"]:\n",
    "        sentence_encode.append(...) # get encoding for this word\n",
    "    sentence_encode = ... # sum over axis=1\n",
    "    encoded_labels.append(...)\n",
    "\n",
    "df[\"one_hot_overview\"] = ...\n",
    "df['one_hot_overview'] # print to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c62e127",
   "metadata": {},
   "source": [
    "**Q2.2.5** Each vector is a vector of floating point (64-bit) numbers. Assuming each float takes up exactly 16-bytes, how many bytes does this take to store (theoretically)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b8235",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b3700",
   "metadata": {},
   "source": [
    "# Q3 Principal Component Analysis\n",
    "\n",
    "**Q3.1** Using the overview encoded into one-hot encodings, perform PCA and plot this into a 2-D image as a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f2be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PCA decomposition class from sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# we did exactly this in the lecture notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63054b6",
   "metadata": {},
   "source": [
    "**Q3.2** Do you see any interpretable structure in the above plot (\"interpretable\" $\\rightarrow$ the patterns are explainable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9409e7b",
   "metadata": {},
   "source": [
    "*Your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4482c5e5",
   "metadata": {},
   "source": [
    "**Q3.3** Quantify how much variance information a 2D PCA projection loses for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338a25d",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da99ba5",
   "metadata": {},
   "source": [
    "**Q3.4** Plot a line graph where the y-axis is cumulative explained variance ratio and the x-axis is the number of PCA components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cac8ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set up PCA object with the desired number of components\n",
    "\n",
    "# fit transform one_hot_overview encodings\n",
    "labels_pca = ...\n",
    "# get explained variance ratio from object, store it in a list\n",
    "...\n",
    "\n",
    "plt.plot(...)\n",
    "# set up y label, x label, title \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b36c9b2",
   "metadata": {},
   "source": [
    "**Q3.5** How many principal components do you need in order to explain 80% of the total variance in the data?\n",
    "\n",
    "Note: don't just estimate it by eye, write some code to compute it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c44b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608db91",
   "metadata": {},
   "source": [
    "*Your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b5c33",
   "metadata": {},
   "source": [
    "**Q3.6** Using the number of dimensions you found in Q3.5, fit and transform your overview encodings using PCA and add it to a new column called `overview_pca` in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6eb40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = ...\n",
    "pca = ...\n",
    "labels_pca = ...\n",
    "df[\"overview_pca\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d3256",
   "metadata": {},
   "source": [
    "# Q4 K-Means Clustering\n",
    "\n",
    "**Q4.1** Cluster the movies based on the features that were extracted via PCA in the last step. Set $K=20$. Add the predicted cluster into the dataframe as a new column called `cluster_kmeans`. Print out the elements of cluster number 0 from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f5781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KMeans class\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = ...\n",
    "# fit model\n",
    "...\n",
    "\n",
    "y_preds = # get predictions\n",
    "df[\"cluster_kmeans\"] = ... # set predictions\n",
    "\n",
    "# print out elements of cluster 0\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34334c6c",
   "metadata": {},
   "source": [
    "**Q4.2** Does this clustering seem alright to you (based on your movie watching history)? Are there movies that go well together and movies that don't?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1d40cc",
   "metadata": {},
   "source": [
    "*Your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0407f9e4",
   "metadata": {},
   "source": [
    "**Q4.3** Now, we'll figure out whether using the elbow method is right for this dataset. Plot a loss (using `kmeans.inertia_`) versus cluster size plot. Is there an elbow that you see clearly? What cluster size would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4984dcd9",
   "metadata": {},
   "source": [
    "*Your text answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "... # your code answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e89e224",
   "metadata": {},
   "source": [
    "# Q5 Gaussian Mixture Models\n",
    "\n",
    "**Q5.1** As with the K-means above, cluster the movies based on the features that were extracted via PCA in a previous step. Set $K=20$. Add the predicted cluster into the dataframe as a new column called `cluster_gmm`. Print out the elements of a single cluster of your choice from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86735028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed02d9",
   "metadata": {},
   "source": [
    "**Q4.2** Does this clustering seem alright to you (based on your movie watching history)? Are there movies that go well together and movies that don't? How does this compare to K-Means Clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e20dd",
   "metadata": {},
   "source": [
    "*Your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0064a1",
   "metadata": {},
   "source": [
    "**Q4.3** Let's check out the size of the clusters. Create a double bar plot (**as you did in the last assignment**) showing the different sizes of the clusters.\n",
    "\n",
    "*Hint: you may want to consider `df.value_counts()` if you're unfamiliar.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array for the x-axis positions\n",
    "# Plotting the bars\n",
    "# Add labels, title, and legend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc124da",
   "metadata": {},
   "source": [
    "**Q4.4** Do you see a significant difference in the sizes of the clusters? Which is more uniform? Any thought on why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f538519",
   "metadata": {},
   "source": [
    "*Your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa60615d",
   "metadata": {},
   "source": [
    "# Q6: A very simple recommender system\n",
    "\n",
    "One useful thing that clustering is often used for (though at a much greater complexity) is in **recommender systems**. These are systems with users and items (movies, files, documents, etc.), where new items are shown to the user based on what they've previously interacted with, and possibly also on the behaviour of other users.\n",
    "\n",
    "**Q6.1** Assume your dentist has just watched the movie Avatar and asks you for a recommendation. Lucky for you, you just finished this assignment. Using the cluster indices of the movie Avatar for both the K-means and GMM methods, print out suggestions for new movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a74f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_watch = \"Avatar\"\n",
    "\n",
    "cluster_kmeans = # find what cluster index Avatar falls into for k-means\n",
    "cluster_gmm = # find what cluster index Avatar falls into for gmm\n",
    "\n",
    "# print out similar movies\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecec1ff",
   "metadata": {},
   "source": [
    "**Q6.2** Are any of the two recommender systems any good? Would you use them if your real dentist asks for a movie suggestion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae96b8a",
   "metadata": {},
   "source": [
    "*Your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b84753",
   "metadata": {},
   "source": [
    "**Q6.3** How would you try making the recommender systems better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d3f6dd",
   "metadata": {},
   "source": [
    "*Your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df2d34a",
   "metadata": {},
   "source": [
    "**Q6.4** Say your dentist likes the movies you suggested and has watched a few more since you met him last. How would you incorporate this fact (recommendation based on multiple movies) into your suggestions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3549de4",
   "metadata": {},
   "source": [
    "*Your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b74ce",
   "metadata": {},
   "source": [
    "**Q6.5** An alternative method for recommendations is to use your encoded movie-feature vectors (in this case your overview+PCA vector) in order to find out what movie may be most similar to the current one. In the case of K-Means and GMMs, \"similarity\" referred to Euclidean distance. However, in this exercise, we will use *cosine similarity*, which is another very common similarity measure, and is related to the angle between two vectors. It is defined as:\n",
    "\n",
    "$$sim(v_1, v_2)=\\frac{v_1\\cdot v_2}{||v_1||\\cdot||v_2||}$$\n",
    "\n",
    "Where $v_1$ and $v_2$ are vectors and the operator $||\\cdot||$ is the norm of the vector. The function ranges from $[-1,1]$ (where 1 means that the vectors point in the same direction).\n",
    "\n",
    "Define an appropriate function `cosine_sim`. Add a column called `cos_sim_to_avatar` in the df and print out the head of the df, sorted by the similarity. What movies would you recommend now? Is this better than the clustering methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(v1, v2):\n",
    "    return ...\n",
    "\n",
    "# get overview_pca encoding of previously watched film\n",
    "prev_watch_enc = ...\n",
    "\n",
    "sims = [] # set up place to store similarities\n",
    "for i, row in df.iterrows():\n",
    "    sims.append(...)\n",
    "\n",
    "df[\"cos_sim_to_avatar\"] = ...\n",
    "df = df.sort_values(...) # sort df\n",
    "df # print to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40391e82",
   "metadata": {},
   "source": [
    "*Your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b1e343",
   "metadata": {},
   "source": [
    "# Q7 Unsupervised Learning as a Tool\n",
    "\n",
    "At the end of this brief forey into natural language processing, we will end with an interesting task: can a machine predict the genre of a movie based purely on the first 10 words of its overview?\n",
    "\n",
    "First, the following cell shuffles the dataset and splits it into a training and test set. The random seed and random state ensure that the train and test samples are the same for you all the time, and *probably* your classmates too. You can verify this by printing out the dataframes and checking it for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n_train = int(0.8*len(df))\n",
    "df_shuffle = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_train = df_shuffle[:n_train].reset_index(drop=True)\n",
    "df_test = df_shuffle[n_train:].reset_index(drop=True)\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9d534",
   "metadata": {},
   "source": [
    "## Train, test, evaluate\n",
    "\n",
    "Using a supervised learning method of your choice, try predicting the genre of a movie from the overview. The way you preprocess your data is up to you, and you can take inspiration from the above exercises (e.g. PCA on the one-hot encodings as input, one-hot label encodings as output). There are a number of hyperparameters you can choose depending on your selected method (number of words in your overview, number of dimensions, number of clusters, hyperparameters of your supervised model...), thus, make sure to perform hyperparameter optimization in some way (grid-search, fine-tuning, etc). Once you are happy with how your model is performing, **print out the train_score (accuracy$\\in[0,1]$), test_score (accuracy$\\in[0,1]$), mean training time (in seconds), and mean inference time (in seconds) of your model**.\n",
    "\n",
    "If you are already excited about neural networks, you may also choose to use that as your supervised method. The easiest way to do it is with sklearn's `MLPClassifier` module. The main hyperparameter you would need to tune is the architecture of your model -- how many hidden layers, and how large is each one. For this task, in order to get best performance, you do not necessarily need an MLP (multi-layer perceptron) but feel free to experiment.\n",
    "\n",
    "*Hint: the `train_and_eval` function from the last assignment should be of great help here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here: feel free to add extra cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5976997e",
   "metadata": {},
   "source": [
    "Write a paragraph of ~150 words about how you went about selecting and tuning your model, and how you may want to make improvements to your model if you were to continue working on this. Plots are very good but not strictly necessary (i.e. try to add plots if you can)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54186a60",
   "metadata": {},
   "source": [
    "*Your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b50007",
   "metadata": {},
   "source": [
    "# BONUS\n",
    "\n",
    "If you are happy with your score and wish to see how well it is doing with respect to other people (a sort of mini-competition), fill out the following form with the train_score (accuracy$\\in[0,1]$), test_score (accuracy$\\in[0,1]$), mean training time (in seconds), and mean inference time (in seconds) of your model, as well as what method you use. You can submit as many times before the deadline as you wish. Your final results will be correlated with your submission in order to validate your results (if we cannot validate them, they will be immediately disqualified).\n",
    "\n",
    "[FORM HERE](https://forms.gle/rXRtXScABH5oDLRWA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
